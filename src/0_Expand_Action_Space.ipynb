{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook expands the action embedding of a pretrained world model\n",
    "to include the new actions introduced to enable the goal-conditioned\n",
    "behavior. The new actions are:\n",
    "1. `ACTION_GET_GOAL` (`env.num_actions`): The following state after this action denotes the desired goal state.\n",
    "2. `ACTION_START_PLANNING` (`env.num_actions + 1`): The following states and actions after this action should be the goal-conditioned trajectory.\n",
    "\n",
    "New checkpoint file is saved to `/path/to/goal-conditioned-iris/src/outputs/checkpoints/epoch_0/last.pt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config file\n",
    "\n",
    "from hydra import initialize, compose\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../config/\"):\n",
    "    cfg = compose(config_name=\"trainer.yaml\")\n",
    "    print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir outputs\n",
    "%cd outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "\n",
    "import hydra\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from agent import Agent\n",
    "from collector import Collector\n",
    "from envs import SingleProcessEnv, MultiProcessEnv\n",
    "from models.actor_critic import ActorCritic\n",
    "from models.world_model import WorldModel\n",
    "from utils import configure_optimizer, EpisodeDirManager, set_seed\n",
    "from planning_utils import expand_world_model_embedding\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, cfg: DictConfig) -> None:\n",
    "        if cfg.common.seed is not None:\n",
    "            set_seed(cfg.common.seed)\n",
    "\n",
    "        self.cfg = cfg\n",
    "        self.start_epoch = 1\n",
    "        self.device = torch.device(cfg.common.device)\n",
    "\n",
    "        self.ckpt_dir = Path(\"checkpoints\")\n",
    "        self.media_dir = Path(\"media\")\n",
    "        self.episode_dir = self.media_dir / \"episodes\"\n",
    "        self.reconstructions_dir = self.media_dir / \"reconstructions\"\n",
    "\n",
    "        if not cfg.common.resume:\n",
    "            config_dir = Path(\"config\")\n",
    "            config_path = config_dir / \"trainer.yaml\"\n",
    "            config_dir.mkdir(exist_ok=True, parents=False)\n",
    "\n",
    "            self.ckpt_dir.mkdir(exist_ok=True, parents=False)\n",
    "            self.media_dir.mkdir(exist_ok=True, parents=False)\n",
    "            self.episode_dir.mkdir(exist_ok=True, parents=False)\n",
    "            self.reconstructions_dir.mkdir(exist_ok=True, parents=False)\n",
    "\n",
    "        episode_manager_train = EpisodeDirManager(\n",
    "            self.episode_dir / \"train\",\n",
    "            max_num_episodes=cfg.collection.train.num_episodes_to_save,\n",
    "        )\n",
    "        episode_manager_test = EpisodeDirManager(\n",
    "            self.episode_dir / \"test\",\n",
    "            max_num_episodes=cfg.collection.test.num_episodes_to_save,\n",
    "        )\n",
    "        self.episode_manager_imagination = EpisodeDirManager(\n",
    "            self.episode_dir / \"imagination\",\n",
    "            max_num_episodes=cfg.evaluation.actor_critic.num_episodes_to_save,\n",
    "        )\n",
    "\n",
    "        def create_env(cfg_env, num_envs):\n",
    "            env_fn = partial(instantiate, config=cfg_env)\n",
    "            return (\n",
    "                MultiProcessEnv(env_fn, num_envs, should_wait_num_envs_ratio=1.0)\n",
    "                if num_envs > 1\n",
    "                else SingleProcessEnv(env_fn)\n",
    "            )\n",
    "\n",
    "        if self.cfg.training.should:\n",
    "            train_env = create_env(cfg.env.train, cfg.collection.train.num_envs)\n",
    "            self.train_dataset = instantiate(cfg.datasets.train)\n",
    "            self.train_collector = Collector(\n",
    "                train_env, self.train_dataset, episode_manager_train\n",
    "            )\n",
    "\n",
    "        if self.cfg.evaluation.should:\n",
    "            test_env = create_env(cfg.env.test, cfg.collection.test.num_envs)\n",
    "            self.test_dataset = instantiate(cfg.datasets.test)\n",
    "            self.test_collector = Collector(\n",
    "                test_env, self.test_dataset, episode_manager_test\n",
    "            )\n",
    "\n",
    "        assert self.cfg.training.should or self.cfg.evaluation.should\n",
    "        env = train_env if self.cfg.training.should else test_env\n",
    "\n",
    "        tokenizer = instantiate(cfg.tokenizer)\n",
    "        world_model = WorldModel(\n",
    "            obs_vocab_size=tokenizer.vocab_size,\n",
    "            act_vocab_size=env.num_actions,\n",
    "            config=instantiate(cfg.world_model),\n",
    "        )\n",
    "        actor_critic = ActorCritic(**cfg.actor_critic, act_vocab_size=env.num_actions)\n",
    "        self.agent = Agent(tokenizer, world_model, actor_critic).to(self.device)\n",
    "        print(\n",
    "            f\"{sum(p.numel() for p in self.agent.tokenizer.parameters())} parameters in agent.tokenizer\"\n",
    "        )\n",
    "        print(\n",
    "            f\"{sum(p.numel() for p in self.agent.world_model.parameters())} parameters in agent.world_model\"\n",
    "        )\n",
    "        print(\n",
    "            f\"{sum(p.numel() for p in self.agent.actor_critic.parameters())} parameters in agent.actor_critic\"\n",
    "        )\n",
    "\n",
    "        if cfg.initialization.path_to_checkpoint is not None:\n",
    "            self.agent.load(**cfg.initialization, device=self.device)\n",
    "\n",
    "        if (\n",
    "            self.agent.world_model.embedder.embedding_tables[0].weight.shape[0]\n",
    "            == self.train_collector.env.num_actions\n",
    "        ):\n",
    "            print(\"Reshaping embedding tables\")\n",
    "            expand_world_model_embedding(self.agent.world_model)\n",
    "\n",
    "        self.optimizer_tokenizer = torch.optim.Adam(\n",
    "            self.agent.tokenizer.parameters(), lr=cfg.training.learning_rate\n",
    "        )\n",
    "        self.optimizer_world_model = configure_optimizer(\n",
    "            self.agent.world_model,\n",
    "            cfg.training.learning_rate,\n",
    "            cfg.training.world_model.weight_decay,\n",
    "        )\n",
    "        self.optimizer_actor_critic = torch.optim.Adam(\n",
    "            self.agent.actor_critic.parameters(), lr=cfg.training.learning_rate\n",
    "        )\n",
    "\n",
    "        if cfg.common.resume:\n",
    "            self.load_checkpoint()\n",
    "\n",
    "    def _save_checkpoint(self, epoch: int, save_agent_only: bool) -> None:\n",
    "        torch.save(self.agent.state_dict(), self.ckpt_dir / \"last.pt\")\n",
    "\n",
    "        if not save_agent_only:\n",
    "            torch.save(epoch, self.ckpt_dir / \"epoch.pt\")\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"optimizer_tokenizer\": self.optimizer_tokenizer.state_dict(),\n",
    "                    \"optimizer_world_model\": self.optimizer_world_model.state_dict(),\n",
    "                    \"optimizer_actor_critic\": self.optimizer_actor_critic.state_dict(),\n",
    "                },\n",
    "                self.ckpt_dir / \"optimizer.pt\",\n",
    "            )\n",
    "\n",
    "            ckpt_epoch_dir = self.ckpt_dir / f\"epoch_{epoch}\"\n",
    "            ckpt_epoch_dir.mkdir(exist_ok=True, parents=False)\n",
    "            torch.save(self.agent.state_dict(), ckpt_epoch_dir / \"last.pt\")\n",
    "            torch.save(epoch, ckpt_epoch_dir / \"epoch.pt\")\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"optimizer_tokenizer\": self.optimizer_tokenizer.state_dict(),\n",
    "                    \"optimizer_world_model\": self.optimizer_world_model.state_dict(),\n",
    "                    \"optimizer_actor_critic\": self.optimizer_actor_critic.state_dict(),\n",
    "                },\n",
    "                ckpt_epoch_dir / \"optimizer.pt\",\n",
    "            )\n",
    "\n",
    "            ckpt_dataset_dir = self.ckpt_dir / \"dataset\"\n",
    "            ckpt_dataset_dir.mkdir(exist_ok=True, parents=False)\n",
    "            self.train_dataset.update_disk_checkpoint(ckpt_dataset_dir)\n",
    "            if self.cfg.evaluation.should:\n",
    "                torch.save(\n",
    "                    self.test_dataset.num_seen_episodes,\n",
    "                    self.ckpt_dir / \"num_seen_episodes_test_dataset.pt\",\n",
    "                )\n",
    "\n",
    "    def save_checkpoint(self, epoch: int, save_agent_only: bool) -> None:\n",
    "        tmp_checkpoint_dir = Path(\"checkpoints_tmp\")\n",
    "        shutil.copytree(\n",
    "            src=self.ckpt_dir,\n",
    "            dst=tmp_checkpoint_dir,\n",
    "            ignore=shutil.ignore_patterns(\"dataset\"),\n",
    "        )\n",
    "        self._save_checkpoint(epoch, save_agent_only)\n",
    "        shutil.rmtree(tmp_checkpoint_dir)\n",
    "\n",
    "    def load_checkpoint(self) -> None:\n",
    "        assert self.ckpt_dir.is_dir()\n",
    "        self.start_epoch = torch.load(self.ckpt_dir / \"epoch.pt\") + 1\n",
    "        self.agent.load(self.ckpt_dir / \"last.pt\", device=self.device)\n",
    "        ckpt_opt = torch.load(self.ckpt_dir / \"optimizer.pt\", map_location=self.device)\n",
    "        self.optimizer_tokenizer.load_state_dict(ckpt_opt[\"optimizer_tokenizer\"])\n",
    "        self.optimizer_world_model.load_state_dict(ckpt_opt[\"optimizer_world_model\"])\n",
    "        self.optimizer_actor_critic.load_state_dict(ckpt_opt[\"optimizer_actor_critic\"])\n",
    "        self.train_dataset.load_disk_checkpoint(self.ckpt_dir / \"dataset\")\n",
    "        if self.cfg.evaluation.should:\n",
    "            self.test_dataset.num_seen_episodes = torch.load(\n",
    "                self.ckpt_dir / \"num_seen_episodes_test_dataset.pt\"\n",
    "            )\n",
    "        print(\n",
    "            f\"Successfully loaded model, optimizer and {len(self.train_dataset)} episodes from {self.ckpt_dir.absolute()}.\"\n",
    "        )\n",
    "\n",
    "    def _to_device(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        return {k: batch[k].to(self.device) for k in batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(epoch=0, save_agent_only=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iris",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
