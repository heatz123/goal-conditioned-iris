{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook evaluates the final checkpoint and creates plots for the planning and imagination on the In-Domain test set.\n",
    "\n",
    "Please make sure that you have recorded your own trajectory, if you want evaluation for the custom dataset. The method for the recording is descripted in the `README.md` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "\n",
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "from omegaconf import OmegaConf\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from einops import rearrange\n",
    "\n",
    "from agent import Agent\n",
    "from collector import Collector\n",
    "from env_collector import EnvCollector\n",
    "from envs import SingleProcessEnv, MultiProcessEnv\n",
    "from episode import Episode\n",
    "from make_reconstructions import make_reconstructions_from_batch\n",
    "from models.actor_critic import ActorCritic\n",
    "from models.world_model import WorldModel\n",
    "from utils import configure_optimizer, EpisodeDirManager, set_seed\n",
    "from planning_utils import expand_world_model_embedding\n",
    "from models.slicer import Embedder, Head\n",
    "from evaluate import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config file\n",
    "\n",
    "# Please set the path as your own path to the last checkpoint\n",
    "PATH_TO_CHECKPOINT = (\n",
    "    \"/path/to/goal-conditioned-iris/src/outputs/checkpoints/epoch_250/last.pt\"\n",
    ")\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../config/\"):\n",
    "    cfg = compose(config_name=\"trainer.yaml\")\n",
    "    cfg.initialization.path_to_checkpoint = PATH_TO_CHECKPOINT\n",
    "    print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd outputs\n",
    "!mkdir eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, cfg: DictConfig) -> None:\n",
    "        wandb.init(\n",
    "            config=OmegaConf.to_container(cfg, resolve=True),\n",
    "            reinit=True,\n",
    "            resume=True,\n",
    "            **cfg.wandb,\n",
    "        )\n",
    "\n",
    "        if cfg.common.seed is not None:\n",
    "            set_seed(cfg.common.seed)\n",
    "\n",
    "        self.cfg = cfg\n",
    "        self.start_epoch = 1\n",
    "        self.device = torch.device(cfg.common.device)\n",
    "\n",
    "        self.ckpt_dir = Path(\"checkpoints\")\n",
    "        self.media_dir = Path(\"media\")\n",
    "        self.episode_dir = self.media_dir / \"episodes\"\n",
    "        self.reconstructions_dir = self.media_dir / \"reconstructions\"\n",
    "\n",
    "        if not cfg.common.resume:\n",
    "            config_dir = Path(\"config\")\n",
    "            config_path = config_dir / \"trainer.yaml\"\n",
    "            config_dir.mkdir(exist_ok=True, parents=False)\n",
    "            wandb.save(str(config_path))\n",
    "\n",
    "            self.ckpt_dir.mkdir(exist_ok=True, parents=False)\n",
    "            self.media_dir.mkdir(exist_ok=True, parents=False)\n",
    "            self.episode_dir.mkdir(exist_ok=True, parents=False)\n",
    "            self.reconstructions_dir.mkdir(exist_ok=True, parents=False)\n",
    "\n",
    "        episode_manager_train = EpisodeDirManager(\n",
    "            self.episode_dir / \"train\",\n",
    "            max_num_episodes=cfg.collection.train.num_episodes_to_save,\n",
    "        )\n",
    "        episode_manager_test = EpisodeDirManager(\n",
    "            self.episode_dir / \"test\",\n",
    "            max_num_episodes=cfg.collection.test.num_episodes_to_save,\n",
    "        )\n",
    "        self.episode_manager_imagination = EpisodeDirManager(\n",
    "            self.episode_dir / \"imagination\",\n",
    "            max_num_episodes=cfg.evaluation.actor_critic.num_episodes_to_save,\n",
    "        )\n",
    "\n",
    "        def create_env(cfg_env, num_envs):\n",
    "            env_fn = partial(instantiate, config=cfg_env)\n",
    "            return (\n",
    "                MultiProcessEnv(env_fn, num_envs, should_wait_num_envs_ratio=1.0)\n",
    "                if num_envs > 1\n",
    "                else SingleProcessEnv(env_fn)\n",
    "            )\n",
    "\n",
    "        if self.cfg.training.should:\n",
    "            train_env = create_env(cfg.env.train, cfg.collection.train.num_envs)\n",
    "            self.train_dataset = instantiate(cfg.datasets.train)\n",
    "            self.train_collector = Collector(\n",
    "                train_env, self.train_dataset, episode_manager_train\n",
    "            )\n",
    "\n",
    "        if self.cfg.evaluation.should:\n",
    "            test_env = create_env(cfg.env.test, cfg.collection.test.num_envs)\n",
    "            self.test_dataset = instantiate(cfg.datasets.test)\n",
    "            self.test_collector = EnvCollector(\n",
    "                test_env, self.test_dataset, episode_manager_test\n",
    "            )\n",
    "\n",
    "        assert self.cfg.training.should or self.cfg.evaluation.should\n",
    "        env = train_env if self.cfg.training.should else test_env\n",
    "\n",
    "        tokenizer = instantiate(cfg.tokenizer)\n",
    "        world_model = WorldModel(\n",
    "            obs_vocab_size=tokenizer.vocab_size,\n",
    "            act_vocab_size=env.num_actions,\n",
    "            config=instantiate(cfg.world_model),\n",
    "        )\n",
    "        actor_critic = ActorCritic(**cfg.actor_critic, act_vocab_size=env.num_actions)\n",
    "        self.agent = Agent(tokenizer, world_model, actor_critic).to(self.device)\n",
    "        print(\n",
    "            f\"{sum(p.numel() for p in self.agent.tokenizer.parameters())} parameters in agent.tokenizer\"\n",
    "        )\n",
    "        print(\n",
    "            f\"{sum(p.numel() for p in self.agent.world_model.parameters())} parameters in agent.world_model\"\n",
    "        )\n",
    "        print(\n",
    "            f\"{sum(p.numel() for p in self.agent.actor_critic.parameters())} parameters in agent.actor_critic\"\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            self.agent.world_model.embedder.embedding_tables[0].weight.shape[0]\n",
    "            == self.train_collector.env.num_actions\n",
    "        ):\n",
    "            print(\"Reshaping embedding tables\")\n",
    "            expand_world_model_embedding(self.agent.world_model)\n",
    "\n",
    "        if cfg.initialization.path_to_checkpoint is not None:\n",
    "            self.agent.load(**cfg.initialization, device=self.device)\n",
    "\n",
    "        self.optimizer_tokenizer = torch.optim.Adam(\n",
    "            self.agent.tokenizer.parameters(), lr=cfg.training.learning_rate\n",
    "        )\n",
    "        self.optimizer_world_model = configure_optimizer(\n",
    "            self.agent.world_model,\n",
    "            cfg.training.learning_rate,\n",
    "            cfg.training.world_model.weight_decay,\n",
    "        )\n",
    "        self.optimizer_actor_critic = torch.optim.Adam(\n",
    "            self.agent.actor_critic.parameters(), lr=cfg.training.learning_rate\n",
    "        )\n",
    "\n",
    "        if cfg.common.resume:\n",
    "            self.load_checkpoint()\n",
    "\n",
    "    def run(self) -> None:\n",
    "        self.train_collector.collect(\n",
    "            self.agent, epoch=1, **self.cfg.collection.train.config\n",
    "        )\n",
    "        self.test_collector.collect(\n",
    "            self.agent, epoch=1, **self.cfg.collection.test.config\n",
    "        )\n",
    "        for epoch in range(self.start_epoch, 1 + self.cfg.common.epochs):\n",
    "            print(f\"\\nEpoch {epoch} / {self.cfg.common.epochs}\\n\")\n",
    "            start_time = time.time()\n",
    "            to_log = []\n",
    "\n",
    "            if self.cfg.training.should:\n",
    "                to_log += self.train_world_model(epoch)\n",
    "\n",
    "            if self.cfg.evaluation.should and (epoch % self.cfg.evaluation.every == 0):\n",
    "                to_log += self.eval_agent(epoch)\n",
    "\n",
    "            if self.cfg.training.should and (epoch % 10 == 0):\n",
    "                self.save_checkpoint(\n",
    "                    epoch, save_agent_only=not self.cfg.common.do_checkpoint\n",
    "                )\n",
    "\n",
    "            to_log.append({\"duration\": (time.time() - start_time) / 3600})\n",
    "            for metrics in to_log:\n",
    "                wandb.log({\"epoch\": epoch, **metrics})\n",
    "\n",
    "        self.finish()\n",
    "\n",
    "    def train_world_model(self, epoch: int) -> None:\n",
    "        self.agent.train()\n",
    "        self.agent.zero_grad()\n",
    "\n",
    "        metrics_tokenizer, metrics_world_model, metrics_actor_critic = {}, {}, {}\n",
    "\n",
    "        cfg_tokenizer = self.cfg.training.tokenizer\n",
    "        cfg_world_model = self.cfg.training.world_model\n",
    "        cfg_actor_critic = self.cfg.training.actor_critic\n",
    "\n",
    "        w = self.cfg.training.sampling_weights\n",
    "\n",
    "        if epoch > cfg_world_model.start_after_epochs:\n",
    "            metrics_world_model = self.train_component(\n",
    "                self.agent.world_model,\n",
    "                self.optimizer_world_model,\n",
    "                sequence_length=self.cfg.common.sequence_length,\n",
    "                sample_from_start=True,\n",
    "                sampling_weights=w,\n",
    "                tokenizer=self.agent.tokenizer,\n",
    "                **cfg_world_model,\n",
    "            )\n",
    "        self.agent.world_model.eval()\n",
    "\n",
    "        return [\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                **metrics_tokenizer,\n",
    "                **metrics_world_model,\n",
    "                **metrics_actor_critic,\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    def get_planning_batch(\n",
    "        self, batch: Dict[str, torch.Tensor], planning_steps\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        ACTION_GET_GOAL = self.train_collector.env.num_actions\n",
    "        ACTION_START_PLANNING = self.train_collector.env.num_actions + 1\n",
    "\n",
    "        new_batch = {k: [] for k in batch}\n",
    "\n",
    "        batch_size, sequence_length = batch[\"mask_padding\"].shape\n",
    "        for batch_idx in range(batch_size):\n",
    "            idx = []\n",
    "\n",
    "            for k in new_batch:\n",
    "                new_batch[k].append([])\n",
    "\n",
    "            # put first 5 steps for the context\n",
    "            for i in range(5):\n",
    "                for k in new_batch:\n",
    "                    new_batch[k][batch_idx].append(batch[k][batch_idx, None, i])\n",
    "\n",
    "            # Check next n steps if there's no termination\n",
    "            i = 5\n",
    "            while len(new_batch[\"observations\"][batch_idx]) < sequence_length - (\n",
    "                planning_steps + 2\n",
    "            ):\n",
    "                if torch.all(\n",
    "                    batch[\"mask_padding\"][batch_idx, i : i + planning_steps]\n",
    "                ) and not torch.any(batch[\"ends\"][batch_idx, i : i + planning_steps]):\n",
    "                    # If there is no termination, add the next n steps to the batch\n",
    "                    for k in batch:\n",
    "                        new_batch[k][batch_idx].append(\n",
    "                            batch[k][batch_idx, None, i].clone()\n",
    "                        )\n",
    "                        new_batch[k][batch_idx].append(\n",
    "                            batch[k][batch_idx, None, i + planning_steps - 1].clone()\n",
    "                        )\n",
    "                        for j in range(i, i + planning_steps - 1):\n",
    "                            new_batch[k][batch_idx].append(\n",
    "                                batch[k][batch_idx, None, j].clone()\n",
    "                            )\n",
    "\n",
    "                    new_batch[\"actions\"][batch_idx][-planning_steps - 1][\n",
    "                        0\n",
    "                    ] = ACTION_GET_GOAL\n",
    "                    new_batch[\"actions\"][batch_idx][-planning_steps][\n",
    "                        0\n",
    "                    ] = ACTION_START_PLANNING\n",
    "                    i += planning_steps - 1\n",
    "\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            # add remainders\n",
    "            while len(new_batch[\"observations\"][batch_idx]) < sequence_length:\n",
    "                for k in batch:\n",
    "                    new_batch[k][batch_idx].append(batch[k][batch_idx, None, i].clone())\n",
    "                i += 1\n",
    "\n",
    "            for k in new_batch:\n",
    "                new_batch[k][batch_idx] = torch.cat(new_batch[k][batch_idx], dim=0)\n",
    "\n",
    "        # convert to tensors\n",
    "        for k in new_batch:\n",
    "            new_batch[k] = torch.stack(new_batch[k])\n",
    "\n",
    "        return new_batch\n",
    "\n",
    "    def train_component(\n",
    "        self,\n",
    "        component: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        steps_per_epoch: int,\n",
    "        batch_num_samples: int,\n",
    "        grad_acc_steps: int,\n",
    "        max_grad_norm: Optional[float],\n",
    "        sequence_length: int,\n",
    "        sampling_weights: Optional[Tuple[float]],\n",
    "        sample_from_start: bool,\n",
    "        **kwargs_loss: Any,\n",
    "    ) -> Dict[str, float]:\n",
    "        loss_total_epoch = 0.0\n",
    "        intermediate_losses = defaultdict(float)\n",
    "\n",
    "        for _ in tqdm(\n",
    "            range(steps_per_epoch), desc=f\"Training {str(component)}\", file=sys.stdout\n",
    "        ):\n",
    "            optimizer.zero_grad()\n",
    "            for _ in range(grad_acc_steps):\n",
    "                batch = self.train_dataset.sample_batch(\n",
    "                    batch_num_samples,\n",
    "                    sequence_length,\n",
    "                    sampling_weights,\n",
    "                    sample_from_start,\n",
    "                )\n",
    "                batch = self.get_planning_batch(batch, self.cfg.common.planning_steps)\n",
    "\n",
    "                batch = self._to_device(batch)\n",
    "\n",
    "                losses = component.compute_loss(batch, **kwargs_loss) / grad_acc_steps\n",
    "                loss_total_step = losses.loss_total\n",
    "                loss_total_step.backward()\n",
    "                loss_total_epoch += loss_total_step.item() / steps_per_epoch\n",
    "\n",
    "                for loss_name, loss_value in losses.intermediate_losses.items():\n",
    "                    intermediate_losses[f\"{str(component)}/train/{loss_name}\"] += (\n",
    "                        loss_value / steps_per_epoch\n",
    "                    )\n",
    "\n",
    "            if max_grad_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(component.parameters(), max_grad_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        metrics = {\n",
    "            f\"{str(component)}/train/total_loss\": loss_total_epoch,\n",
    "            **intermediate_losses,\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_agent(self, epoch: int) -> None:\n",
    "        self.agent.eval()\n",
    "\n",
    "        metrics_tokenizer, metrics_world_model = {}, {}\n",
    "\n",
    "        cfg_tokenizer = self.cfg.evaluation.tokenizer\n",
    "        cfg_world_model = self.cfg.evaluation.world_model\n",
    "        cfg_actor_critic = self.cfg.evaluation.actor_critic\n",
    "\n",
    "        if epoch > cfg_world_model.start_after_epochs:\n",
    "            metrics_world_model = evaluate(\n",
    "                self.test_collector.env,\n",
    "                self.test_dataset,\n",
    "                self.agent,\n",
    "                self.cfg.common.planning_steps,\n",
    "                save_plots=True,\n",
    "            )\n",
    "            metrics_random = evaluate(\n",
    "                self.test_collector.env,\n",
    "                self.test_dataset,\n",
    "                self.agent,\n",
    "                self.cfg.common.planning_steps,\n",
    "                save_plots=False,\n",
    "                random=True,\n",
    "            )\n",
    "            metrics_world_model[\"random/eval/goal_distance\"] = metrics_random[\n",
    "                \"world_model/eval/goal_distance\"\n",
    "            ]\n",
    "\n",
    "        return [metrics_world_model]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_component(\n",
    "        self,\n",
    "        component: nn.Module,\n",
    "        batch_num_samples: int,\n",
    "        sequence_length: int,\n",
    "        **kwargs_loss: Any,\n",
    "    ) -> Dict[str, float]:\n",
    "        loss_total_epoch = 0.0\n",
    "        intermediate_losses = defaultdict(float)\n",
    "\n",
    "        steps = 0\n",
    "        pbar = tqdm(desc=f\"Evaluating {str(component)}\", file=sys.stdout)\n",
    "        for batch in self.test_dataset.traverse(batch_num_samples, sequence_length):\n",
    "            batch = self.get_planning_batch(batch, self.cfg.common.planning_steps)\n",
    "            batch = self._to_device(batch)\n",
    "\n",
    "            losses = component.compute_loss(batch, **kwargs_loss)\n",
    "            loss_total_epoch += losses.loss_total.item()\n",
    "\n",
    "            for loss_name, loss_value in losses.intermediate_losses.items():\n",
    "                intermediate_losses[f\"{str(component)}/eval/{loss_name}\"] += loss_value\n",
    "\n",
    "            steps += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "        intermediate_losses = {k: v / steps for k, v in intermediate_losses.items()}\n",
    "        metrics = {\n",
    "            f\"{str(component)}/eval/total_loss\": loss_total_epoch / steps,\n",
    "            **intermediate_losses,\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "    def _save_checkpoint(self, epoch: int, save_agent_only: bool) -> None:\n",
    "        torch.save(self.agent.state_dict(), self.ckpt_dir / \"last.pt\")\n",
    "\n",
    "        if not save_agent_only:\n",
    "            torch.save(epoch, self.ckpt_dir / \"epoch.pt\")\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"optimizer_tokenizer\": self.optimizer_tokenizer.state_dict(),\n",
    "                    \"optimizer_world_model\": self.optimizer_world_model.state_dict(),\n",
    "                    \"optimizer_actor_critic\": self.optimizer_actor_critic.state_dict(),\n",
    "                },\n",
    "                self.ckpt_dir / \"optimizer.pt\",\n",
    "            )\n",
    "\n",
    "            ckpt_epoch_dir = self.ckpt_dir / f\"epoch_{epoch}\"\n",
    "            ckpt_epoch_dir.mkdir(exist_ok=True, parents=False)\n",
    "            torch.save(self.agent.state_dict(), ckpt_epoch_dir / \"last.pt\")\n",
    "            torch.save(epoch, ckpt_epoch_dir / \"epoch.pt\")\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"optimizer_tokenizer\": self.optimizer_tokenizer.state_dict(),\n",
    "                    \"optimizer_world_model\": self.optimizer_world_model.state_dict(),\n",
    "                    \"optimizer_actor_critic\": self.optimizer_actor_critic.state_dict(),\n",
    "                },\n",
    "                ckpt_epoch_dir / \"optimizer.pt\",\n",
    "            )\n",
    "\n",
    "            ckpt_dataset_dir = self.ckpt_dir / \"dataset\"\n",
    "            ckpt_dataset_dir.mkdir(exist_ok=True, parents=False)\n",
    "            self.train_dataset.update_disk_checkpoint(ckpt_dataset_dir)\n",
    "            if self.cfg.evaluation.should:\n",
    "                torch.save(\n",
    "                    self.test_dataset.num_seen_episodes,\n",
    "                    self.ckpt_dir / \"num_seen_episodes_test_dataset.pt\",\n",
    "                )\n",
    "\n",
    "    def save_checkpoint(self, epoch: int, save_agent_only: bool) -> None:\n",
    "        tmp_checkpoint_dir = Path(\"checkpoints_tmp\")\n",
    "        shutil.copytree(\n",
    "            src=self.ckpt_dir,\n",
    "            dst=tmp_checkpoint_dir,\n",
    "            ignore=shutil.ignore_patterns(\"dataset\"),\n",
    "        )\n",
    "        self._save_checkpoint(epoch, save_agent_only)\n",
    "        shutil.rmtree(tmp_checkpoint_dir)\n",
    "\n",
    "    def load_checkpoint(self) -> None:\n",
    "        assert self.ckpt_dir.is_dir()\n",
    "        self.start_epoch = torch.load(self.ckpt_dir / \"epoch.pt\") + 1\n",
    "        self.agent.load(self.ckpt_dir / \"last.pt\", device=self.device)\n",
    "        ckpt_opt = torch.load(self.ckpt_dir / \"optimizer.pt\", map_location=self.device)\n",
    "        self.optimizer_tokenizer.load_state_dict(ckpt_opt[\"optimizer_tokenizer\"])\n",
    "        self.optimizer_world_model.load_state_dict(ckpt_opt[\"optimizer_world_model\"])\n",
    "        self.optimizer_actor_critic.load_state_dict(ckpt_opt[\"optimizer_actor_critic\"])\n",
    "        self.train_dataset.load_disk_checkpoint(self.ckpt_dir / \"dataset\")\n",
    "        if self.cfg.evaluation.should:\n",
    "            self.test_dataset.num_seen_episodes = torch.load(\n",
    "                self.ckpt_dir / \"num_seen_episodes_test_dataset.pt\"\n",
    "            )\n",
    "        print(\n",
    "            f\"Successfully loaded model, optimizer and {len(self.train_dataset)} episodes from {self.ckpt_dir.absolute()}.\"\n",
    "        )\n",
    "\n",
    "    def _to_device(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        return {k: batch[k].to(self.device) for k in batch}\n",
    "\n",
    "    def finish(self) -> None:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(cfg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In-domain Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.collection.test.config.num_episodes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_log = trainer.test_collector.collect(\n",
    "    trainer.agent, epoch=1, **trainer.cfg.collection.test.config\n",
    ")\n",
    "print(to_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_log = trainer.eval_agent(epoch=2)\n",
    "print(to_log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the `src/outputs/eval` folder to see the planning / imagination plots."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OOD Evaluation (Evaluation on Custom Dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note you have to create your own trajectory in advance. Please refer to `README.md` for the guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test_dataset.clear()\n",
    "trainer.test_dataset.load_custom_trajectories(Path(\"../../custom_trajectories/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_log = trainer.eval_agent(epoch=3)\n",
    "print(to_log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iris",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
